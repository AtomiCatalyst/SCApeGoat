{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Side Channel Analysis Metric API\n",
    "There exists very few comprehensive open-source libraries for side-channel analysis. Libraries that do exist are difficult to use and lack the proper documentation. This Jupyter Notebook outlines some of the most useful metrics when conducting side channel analysis in an easy to understand medium. These metrics do not perform an attack, rather they are used to assess gain insight into the cryptographic system. Many of the metrics in this API require data to be pre-formatted prior to use. However, each metric explains what programmer needs to provide. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f9edd07ef33dc4d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_13096\\383004719.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstats\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\stats\\_stats_py.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(x, y, alternative, method)\u001B[0m\n\u001B[0;32m   4817\u001B[0m     \u001B[1;31m# more precision if the input is, for example, np.longdouble.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4818\u001B[0m     \u001B[0mdtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1.0\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4819\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4820\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4821\u001B[1;33m         \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msign\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msign\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4822\u001B[0m         result = PearsonRResult(statistic=r, pvalue=1.0, n=n,\n\u001B[0;32m   4823\u001B[0m                                 alternative=alternative, x=x, y=y)\n\u001B[0;32m   4824\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:49:27.602625400Z",
     "start_time": "2023-11-30T05:49:26.995980400Z"
    }
   },
   "id": "18d57964c6d725bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Signal-to-Noise Ratio \n",
    "The signal-to-noise ratio of a signal is defined as the ratio of a signal's data component to the signal's noise component. For side-channel analysis, the SNR of a power trace relates to the ability for an attacker to obtain information from a power trace during an attack. The effectiveness of side channel attack increases for larger SNR values since the signal leakage is more prominent relative to the noise of the signal. Typically recorded power traces need to be partitioned into different sets called labels. \n",
    "\n",
    "$$\n",
    "SNR = \\frac{VAR(L_d)}{VAR(L_n)} = \\frac{\\sum_{v=0}^{V} (\\hat{\\mu_v}^2 - \\hat{\\mu})^2}{\\hat{\\sigma}^2}\n",
    "$$\n",
    "The resulting array is the value of the SNR at a given discrete time sample. Windows of the resulting trace where the magnitude of the SNR is high may also indicate an area of interest since it implies that there exists a significant amount of leakage at that sample."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec9b8e1b8e3bf69b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Signal to Noise Ratio metric \n",
    "#   - labels: An array of arrays. Each index of the labels array (i.e. labels[0])\n",
    "#     is a label group containing the corresponding power traces.\n",
    "# return: the SNR signal \n",
    "def signal_to_noise_ratio(labels):\n",
    "   # statistical mean and variances of each set \n",
    "   set_means = []\n",
    "   set_variances = []\n",
    "   \n",
    "   for label in labels:\n",
    "       set_means.append(np.mean(label, axis=0)) # take the mean along the column\n",
    "       set_variances.append(np.var(label, axis=0)) # take the variance along the column\n",
    "   \n",
    "   # calculate overall mean and variance\n",
    "   overall_mean= np.mean(set_means, axis=0)              \n",
    "   overall_variance = np.var(set_variances, axis=0)\n",
    "\n",
    "   # perform SNR calculation\n",
    "   l_d = np.zeros(len(set_means[0]))\n",
    "   for mean in set_means:\n",
    "       l_d = np.add(l_d, np.square(np.subtract(mean, overall_mean)))\n",
    "   l_n = overall_variance\n",
    "\n",
    "   snr = np.divide(l_d, l_n)\n",
    "\n",
    "   return snr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:52:12.603279300Z",
     "start_time": "2023-11-24T01:52:12.478555900Z"
    }
   },
   "id": "303cf73826670303"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Score and Rank\n",
    "The score and rank metric is a helpful metric to use both during an attack and in the analysis of a system. This metric relies primarily on two steps, first, the full-length cryptographic key is split into multiple segments, called partitions. Typically, these partitions are the size of a byte, but can be either larger or shorter depending on the particular encryption algorithm and user implementation. This means that for partitions that are the size of a byte, there are 256 key possibilities. Next, a scoring function needs to be specified. This function is arbitrary but needs to return a numerical scores such that the higher the score, the more likely a given input key, k, actually produced the traces. Using the scoring function, for each partition, each possible key is ranked from the highest score to the lowest score. The idea of ranking and scoring the key guesses is that as the number of traces increases, the rank will converge to the point that the actual key will remain in the 1st rank, or very close to it for all key partitions. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a63106a69955bd04"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Score metric: Ranks each key guess in a key partition based on a scoring function\n",
    "#   - traces: The trace set to be evaluated\n",
    "#   - score_fcn: Function callback that takes two arguments, traces and a guess candidate\n",
    "#                and returns a \"score\" such that the higher the value, the more likely the \n",
    "#                key candidate is the actual key\n",
    "#   - partitions: The number of partitions of the key full key.\n",
    "# return: A 2D array rank. The value rank[i] are the key guess rankings for partition i. \n",
    "#         The value of rank[i][0] is the highest ranked key guess for partition i.\n",
    "def score_and_rank(traces, score_fcn, key_candidates, partitions):\n",
    "        dtype = [('key', int), ('score', 'float64')]\n",
    "        ranks = []\n",
    "        # for each key partition        \n",
    "        for i in range(partitions): \n",
    "            partition_scores = np.array([], dtype=dtype)\n",
    "            \n",
    "            # for each key guess in the partition score the value and add to list\n",
    "            for k in key_candidates:\n",
    "                score_k = score_fcn(traces, k)\n",
    "                key_score = np.array([(k, score_k)], dtype=dtype)\n",
    "                partition_scores = np.append(partition_scores, key_score)\n",
    "                \n",
    "            # rank each key where partition_ranks[0] is the key that scored the highest\n",
    "            partition_ranks = np.array([key_score[0] for key_score in np.sort(partition_scores, order='score')[::-1]])\n",
    "            \n",
    "            ranks.append(partition_ranks)\n",
    "        return ranks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:52:12.605175600Z",
     "start_time": "2023-11-24T01:52:12.498484300Z"
    }
   },
   "id": "45dde6602bcdcbc7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Success Rate\n",
    "In the analysis of a system, the Success Rate metric can be used alongside the Score and Rank metrics in order to help determine the security of a system. This metric is typically conducted by an evaluator not an attacker since the correct key for the cryptographic system must be known. The success rate of a given experiment, i, is defined as 1 if the correct key is ranked within the top o key guesses. The order, o, can be any value between 1 and K where K is the number of key guesses. When o is 1, the success rate is only 1 if the correct key was ranked first out of all possible key guesses. Similarly, if o is 2, the success rate will be 1 if the correct key is ranking within the top 2 ranks. \n",
    "\\begin{equation}\n",
    "    SR_{o}^{i}=\n",
    "    \\begin{cases}\n",
    "        \\text{1 if } k_{c} \\in [guess_{1}, guess_{2}, ..., guess_{o}]\\\\\n",
    "        \\text{0 otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "The overall success rate is defined as the sum of the success rates of all experiments divided by the number of experiments. Lower success rates indicate a high degree of system security and vise versa. \n",
    "\\begin{equation}\n",
    "    SR_{o}= \\frac{1}{p}\\sum_{i=1}^{p}SR_{o}^{i}\n",
    "\\end{equation}\n",
    "\n",
    "## Guessing Entropy\n",
    "Guessing Entropy is another similar metric that can analyze the security of a system. The Guessing Entropy is defined as the sum of the natural log of the rank of the correct key for all experiments. \n",
    "\\begin{equation}\n",
    "    GE^{i}=log_{2}(rank_{k_{c}})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    GE = \\frac{1}{p}\\sum_{i=1}^{p}GE^{i}\n",
    "\\end{equation}\n",
    "The guessing entropy metric conveys the average workload left in the attack. As the guessing entropy decreases the certainty of the key guess increases, to the point where at a GE of 0, the key guess is certain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa1506f97238090e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d33245a26bb13225"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Success Rate and Guessing Entropy Metric\n",
    "#   - correct_key: the correct key of the cryptographic system, typically a key partition in practice\n",
    "#   - ranks: The ranks of key guesses for a given experiment\n",
    "#   - num_experiments: The number of experiments conducted\n",
    "# return: The values of success_rate and guessing_entropy for the given number of experiments\n",
    "def success_rate_guessing_entropy(correct_key, ranks, order, num_experiments):\n",
    "    success_rate = 0\n",
    "    guessing_entropy = 0\n",
    "    \n",
    "    # for each experiment\n",
    "    for i in range(num_experiments):\n",
    "        \n",
    "        # check if correct key is within o ranks\n",
    "        for j in range(order):\n",
    "            if ranks[i][j] == correct_key:\n",
    "                success_rate += 1\n",
    "                break\n",
    "        \n",
    "        # guessing entropy is the log2 of the rank of the correct key\n",
    "        guessing_entropy += math.log2(ranks[i].index(correct_key) + 1)\n",
    "    \n",
    "    success_rate = success_rate / num_experiments\n",
    "    guessing_entropy = guessing_entropy / num_experiments\n",
    "    \n",
    "    return success_rate, guessing_entropy\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:52:12.605175600Z",
     "start_time": "2023-11-24T01:52:12.521357600Z"
    }
   },
   "id": "3a8f43e33d17956"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pearson Correlation Coefficient\n",
    "Correlation can be used as a metric that compares two different trace sets. The first set of power traces are observed leakages relating to an intermediate value V. The second set are predicted power traces that were derived using some sort of leakage model g(.) relating to an intermediate algorithm output value V for the correct key guess.\n",
    "$$\n",
    "p_{k_{c}} = \\frac{\\sum_{i=1}^{n} (l_{i}- \\frac{1}{n} \\sum_{i=1}^{n} l_{i}) (g(f(x_{i},k_{c})) - \\frac{1}{n} \\sum_{i=1}^{n} g(f(x_{i},k_{c})))}{{\\sqrt{\\sum_{i=1}^{n}({l_i - \\frac{1}{n}\\sum_{i=1}^n}{l_i})^2} \\sqrt{\\sum_{i=1}^n{(g(f(x_{i},k_{c}))-\\frac{1}{n} \\sum_{i=1}^{n}{g(f(x_{i},k_{c}))})^2}}}}\n",
    "$$\n",
    "If the absolute value of the correlation between predicted traces modeled using the correct key, is greater than that of other key guesses, then the key will likely be able to be derived from a side channel attack. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68bf175bf51a034b"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Pearson's Correlation Coefficient Metric\n",
    "#   - predicted_leakage: predicted traces associated with intermediate values and a key guess and a plaintext value\n",
    "#   - observed_leakage: actual power traces observed with a given plaintext\n",
    "# returns: the correlation coefficient and p-value between each trace\n",
    "# TODO: Write an implementation similar to the example in the hardware hacking handbook\n",
    "def pearson_correlation(predicted_leakage, observed_leakage):\n",
    "    correlation_coeff, p_value = stats.pearsonr(predicted_leakage, observed_leakage)\n",
    "    return abs(correlation_coeff), p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:56:14.899061100Z",
     "start_time": "2023-11-24T01:56:14.829355200Z"
    }
   },
   "id": "a9557b50f10b2111"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## T-Test with TVLA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8f4e206539311d1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# T-test with TVlA metric:  In general |t| > th where th = 4.5 means that the system leaks information about the cryptographic key.\n",
    "#   - fixed:  Trace set recorded with a fixed plaintext \n",
    "#   - random: Trace set recorded with a random set of plaintexts\n",
    "# return: t_statistic and p-value\n",
    "def t_test_tvla(fixed, random):\n",
    "    \n",
    "    # determine t_statistic and p-value using scipy \n",
    "    t_statistic, p_value = stats.ttest_ind(fixed, random)\n",
    "    \n",
    "    # high t-statistic and low p-values indicate that a given time sample leaks information\n",
    "    return t_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T01:52:14.202190600Z",
     "start_time": "2023-11-24T01:52:14.088828400Z"
    }
   },
   "id": "bd9e04554a871567"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
